# PostgreSQL Database Setup Guide

## Quick Setup Steps

### Step 1: Add Your PostgreSQL URL

1. Open `c:\TruEstate\backend\.env`
2. Add this line with YOUR complete PostgreSQL URL:
   ```env
   POSTGRES_URL=postgresql://your_complete_url_here
   ```

**Important:** Make sure you have the COMPLETE URL including the full hostname (e.g., `.render.com`, `.neon.tech`, etc.)

Example:
```env
POSTGRES_URL=postgresql://user:password@dpg-xxxxx.oregon-postgres.render.com/database
```

### Step 2: Install PostgreSQL Package

```bash
npm install
```

### Step 3: Create Database Schema

Run this command to create the `sales` table and indexes:

```bash
node -e "const fs=require('fs');const{Pool}=require('pg');require('dotenv').config();const pool=new Pool({connectionString:process.env.POSTGRES_URL,ssl:{rejectUnauthorized:false}});const sql=fs.readFileSync('./database/schema.sql','utf8');pool.query(sql).then(()=>console.log('âœ… Schema created successfully!')).catch(e=>console.error('Error:',e.message)).finally(()=>pool.end());"
```

You should see:
```
âœ… Schema created successfully!
```

### Step 4: Migrate CSV Data to PostgreSQL

Run this command to import all 1M records from CSV:

```bash
node database/migrate.js
```

Expected output:
```
ðŸš€ Starting CSV to PostgreSQL migration...
ðŸ“ Reading CSV from: C:\TruEstate\sales_data.csv
ðŸ—‘ï¸  Cleared existing data
âœ… Inserted batch 1 (1000 records)
âœ… Inserted batch 2 (1000 records)
...
âœ… Migration completed successfully!
ðŸ“Š Total records migrated: 1,000,000
âœ… Verification: 1000000 records in database
```

â±ï¸ This will take 2-5 minutes depending on your internet connection.

### Step 5: Start the Backend

```bash
npm run dev
```

You should see:
```
Testing PostgreSQL connection...
âœ… Database connected successfully!
ðŸš€ Server running on port 5000
```

### Step 6: Test the Application

1. Open the frontend: http://localhost:5173
2. Try searching for a customer
3. Apply filters
4. Sort the data
5. Navigate through pages

You should notice significantly faster response times!

## Verification

### Check Database Connection
```bash
node -e "const {pool} = require('./src/config/db.js'); pool.query('SELECT COUNT(*) FROM sales').then(r => console.log('Records:', r.rows[0].count)).catch(console.error).finally(() => pool.end());"
```

### Check Indexes
```sql
SELECT indexname FROM pg_indexes WHERE tablename = 'sales';
```

Should show 10+ indexes including:
- idx_customer_name
- idx_customer_search (GIN index for full-text search)
- idx_customer_region
- idx_date
- idx_filter_combo
- etc.

## Performance Comparison

| Operation | Before (CSV) | After (PostgreSQL) |
|-----------|-------------|-------------------|
| Search | ~500ms | ~20-50ms |
| Filter | ~800ms | ~30-80ms |
| Sort | ~1000ms | ~10ms (indexed) |
| Combined | ~1500ms | ~50-100ms |

## Troubleshooting

**Error: "relation 'sales' does not exist"**
- Run the schema.sql file first (Step 3)

**Error: "Connection refused"**
- Check your POSTGRES_URL is correct
- Ensure PostgreSQL server is running

**Error: "authentication failed"**
- Verify username and password in POSTGRES_URL

**Migration script hangs**
- Check CSV file path is correct
- Ensure you have enough disk space
- Check PostgreSQL connection

## Rollback (if needed)

To go back to CSV-based approach:
1. Stop the backend
2. Restore the old service files from git
3. Remove POSTGRES_URL from .env
4. Restart backend

The CSV data is unchanged and can still be used.
